# -*- coding: utf-8 -*-
"""LSTM using tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rEcVRvj0UraH_ziONv3jdbX1lvsnfxmB
"""

# To mount the google drive to google colab:

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd

# read the dataset:

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv', error_bad_lines=False, engine='python')

df.head()

# check for the null values:

df.isnull().sum()

# drop all the NaN values:
df = df.dropna()

df.head()

# checking the unique labels:

df["label"].unique()

df["label"].value_counts()

# drop all the rows which doesnt not contain 0 or 1:
df = df[df["label"].isin(['0','1'])]

df["label"].value_counts()

df.shape

# Get the independent features:

X = df.drop("label", axis=1)

# Get the dependent features:
y = df["label"]

print("Checking the shape of dataframe", X.shape)

print("Checking the shape of y ", y.shape)

pip install tensorflow==2.8.2

import tensorflow as tf

tf.__version__

# import the tensorflow libraries:

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Define the vocab size:

voc_size = 5000

# One hot representation:

messages = X.copy()

messages["title"][1]

messages

# reset the index:
messages.reset_index(inplace=True)

messages

import nltk
import re
from nltk.corpus import stopwords


nltk.download("stopwords")

messages

# data preprocessing:

from nltk.stem.porter import PorterStemmer # for the stemming purpose

ps = PorterStemmer()
corpus = []
for i in range(0, len(messages)):
  review = re.sub('[^a-zA-Z]', ' ', messages["title"][i])
  review = review.lower()
  # split the sentences based on the space:
  review = review.split()

  review = [ps.stem(word) for word in review if not word in stopwords.words("english")]
  review = ' '.join(review)
  corpus.append(review)

corpus

# checking an example of corpus:
corpus[1]

onehot_repr = [one_hot(words, voc_size) for words in corpus]

onehot_repr[1]

# Embedding Representation:

sent_length = 20
embedded_docs = pad_sequences(onehot_repr, padding="post", maxlen = sent_length)
print(embedded_docs)

embedded_docs[1]

embedded_docs[0]

# Creating a model:

embedding_vector_features = 40
model = Sequential()
model.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))
model.add(LSTM(100))
model.add(Dense(1, activation="sigmoid"))
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
print(model.summary())

len((embedded_docs)), y.shape

import numpy as np
X_final = np.array(embedded_docs)
y_final = np.array(y)

X_final.shape, y_final.shape

y.unique()

y = y.astype(int)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final,test_size=0.33, random_state=42)

y_test

# Model Training:

model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, batch_size=64)

# Adding a  Dropout:

from tensorflow.keras.layers import Dropout
# creating model:

embedding_vector_features = 40
model = Sequential()
model.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))
model.add(Dropout(0.3))
model.add(LSTM(100))
model.add(Dropout(0.3))
model.add(Dense(1, activation="sigmoid"))
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Performance metrics and accuracy:

y_pred = model.predict(X_test)

y_pred = np.where(y_pred > 0.6, 1, 0) ### AUC ROC Curve

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_pred)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

